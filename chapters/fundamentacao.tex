% O comando \phantomsection é importante para a correta geração de links pelo hyperref.
\phantomsection

% O \chapter* usa um asterisco para que não seja numerado automaticamente, 
% caso seu orientador prefira assim. Se quiser numerado (ex: "Capítulo 2"), 
% use apenas \chapter{Fundamentação Teórica}.
\chapter{Fundamentação Teórica}
\label{ch:fundamentacao-teorica}

% ---
\section{Introdução ao Capítulo}
\label{sec:fund-intro}
% Escreva aqui um parágrafo introdutório que apresenta os objetivos e a estrutura deste capítulo.
% Ex: "Este capítulo estabelece a base conceitual para o presente trabalho, iniciando pela discussão
% sobre o direito à privacidade e o arcabouço legal da LGPD, passando pelas técnicas de
% anonimização de dados clínicos e concluindo com as métricas utilizadas para avaliar o
% balanço entre a proteção da privacidade e a utilidade dos dados para pesquisa."
% ---

% ---
\section{Privacidade e Proteção de Dados na Saúde: O Cenário da LGPD}
\label{sec:fund-lgpd}

\subsection{O Conceito de Privacidade na Era Digital}
\label{subsec:fund-privacidade}
Em uma das primeiras definições de privacidade, Samuel Warren e Louis Brandeis \cite{WarrenBrandeis1890} articularam a existência de um "direito a ser deixado em paz" (\textit{the right to be let alone}). O argumento era de que a proteção de pensamentos, sentimentos e emoções expressos por meio da escrita ou das artes contra plágio ou apropriação física não era um princípio da propriedade privada (sob o qual a propriedade intelectual é baseada), mas sim um direito a "inviolabilidade pessoal", e portanto não somente  as produções intelectuais ou artísticas, mas também as expressões casuais do dia a dia — pensamentos, emoções e ações - deveriam ser protegidas da exposição pública indesejada. Com isso, a privacidade foi estabelecida como um direito universal que não deriva de um contrato ou de uma relação de propriedade, mas sim como um princípio inerente à dignidade humana, lançando as bases para todo o debate futuro sobre o tema.

Décadas depois, já no início da era da computação, em \textit{Privacy and Freedom}, Alan Westin analisou como as novas tecnologias de vigilância e a capacidade de armazenamento e processamento de dados em computadores, criavam uma ameaça inédita à liberdade individual. Diante desse cenário, Westin formulou sua influente definição de privacidade como o direito de um indivíduo de determinar quando, como e em que medida as informações sobre si são comunicadas a terceiros \cite{Westin1967}. Essa noção, conhecida como "autodeterminação informativa", deslocou o foco de um direito passivo ao isolamento para um poder ativo de controle sobre o fluxo de dados pessoais. É este o princípio que constitui a base da Lei Geral de Proteção de Dados Pessoais (LGPD) brasileira \cite{Brasil2018lgpd}. A LGPD, ao estabelecer as bases legais para o tratamento de dados, como o consentimento informado e a finalidade legítima, materializa a visão de Westin: ela não proíbe o uso de dados, mas o regulamenta para garantir que o titular mantenha o controle sobre suas informações, transformando um princípio filosófico em um direito legal e aplicável.

Outra definição fundamental para este trabalho, que avança em relação à noção de controle individual, é a de Integridade Contextual, proposta por Helen Nissenbaum. Em sua obra \textit{Privacy in Context}, a autora argumenta que a privacidade não é simplesmente sobre manter informações secretas, mas sim sobre garantir que o fluxo de informações pessoais siga as normas esperadas para um determinado contexto social \cite{Nissenbaum2009}. A violação da privacidade ocorre quando essas normas são quebradas. Nissenbaum modela essas normas a partir de três parâmetros: os atores (quem envia, quem recebe), os atributos (o tipo de informação) e os princípios de transmissão (as regras que governam o fluxo). O contexto da saúde pode ser um exemplo: um paciente (ator) espera que seu diagnóstico (atributo) seja compartilhado com outro especialista (ator) sob um princípio de confidencialidade, mas ficaria chocado se o mesmo dado fosse vendido a uma empresa de marketing, pois isso violaria as normas contextuais.

% Para a presente pesquisa, o conceito de Nissenbaum é crucial, pois justifica a anonimização não como uma perda de controle, mas como uma mudança de contexto: ao anonimizar os dados clínicos, removemos os atores originais (pacientes) para adequar a informação a um novo contexto — o da pesquisa científica —, com novos atores (pesquisadores) e novos princípios de transmissão (uso para o bem comum), mantendo assim a integridade do fluxo informacional.

Finalmente, a discussão sobre privacidade na era digital seria incompleta sem analisar a lógica econômica que a desafia em sua essência. Em sua obra \textit{A Era do Capitalismo de Vigilância}, a economista Shoshana Zuboff argumenta que a massiva coleta de dados por empresas de tecnologia não é um simples efeito colateral, mas a fundação de uma nova forma de mercado \cite{Zuboff2019}. Nesse modelo, as experiências humanas são tratadas como matéria-prima gratuita e processadas como "excedente comportamental". Este excedente é utilizado para fabricar "produtos de predição", que são vendidos em novos mercados com o objetivo de antecipar e até mesmo influenciar o comportamento dos usuários. Essa lógica econômica choca-se frontalmente com as definições anteriores: ela torna o ideal de controle individual de Westin uma ilusão, ao operar por meio da extração massiva e da ofuscação, e viola sistematicamente a integridade contextual de Nissenbaum, ao remover os dados de seu contexto original para alimentar outros mercados de futuros comportamentais. Com essa nova lógica mercadológica, a questão da privacidade se transforma de uma violação individual para um desafio sistêmico ao qual a legislação precisa se adaptar. A resposta legal começa por categorizar e definir precisamente o objeto de proteção, o que nos leva à distinção fundamental entre o dado pessoal e o dado sensível.

\subsection{O Dado Pessoal e o Dado Sensível}
\label{subsec:fund-dados-sensiveis}

O Art. 5º, inciso I, da LGPD \cite{Brasil2018lgpd} define dado pessoal como "informação relacionada a pessoa natural identificada ou identificável". Essa definição é propositalmente ampla, abrangendo não apenas dados que apontam diretamente para um indivíduo, como nome completo ou CPF, mas também informações que, isoladamente ou em combinação com outras, podem levar à identificação de uma pessoa. Exemplos incluem endereços IP, dados de geolocalização, ou mesmo uma combinação de atributos como profissão, cidade e idade.

No inciso seguinte do mesmo artigo (Art. 5º, II), a LGPD qualifica o dado sensível, um subconjunto especial de dados pessoais que, devido à sua natureza, requer um nível mais elevado de proteção. Entre as informações destacadas como dados sensíveis pela legislação, estão "dados referentes à saúde" e "dados genéticos ou biométricos". A inclusão desses tipos de dados na categoria de sensíveis reflete o reconhecimento de que sua exposição pode levar a discriminação, estigmatização ou outros danos significativos ao titular. Por exemplo, a divulgação não autorizada de um diagnóstico médico pode afetar a vida pessoal e profissional de um indivíduo, enquanto dados genéticos podem revelar predisposições a certas doenças que podem impactar decisões de seguro ou emprego.

A definição de dado sensível como posta na legislação (por meio de exemplificação) não é exaustiva, e que para determinar um dado como sensível é essencial verificar o contexto de sua utilização e sua relação com outras informações disponíveis. Dessa forma, deve-se admitir que outros dados, não explicitamente mencionados como sensíveis, podem ser considerados como tal a depender do uso que se faz deles e sua potencialidade de se tornar instrumentos de discriminação ou violação de direitos fundamentais \cite{TefferViola2020}. Isso reforça a necessidade de um cuidado redobrado no tratamento de dados pessoais na área da saúde, que mesmo quando não categorizados formalmente como sensíveis, podem revelar aspectos íntimos da vida dos pacientes.

\subsection{Pilares da LGPD: Consentimento e Transparência}
\label{subsec:fund-consentimento}

O Art. 7º da LGPD é dedicado às hipóteses que autorizam o tratamento de dados pessoais. Entre essas, o inciso I destaca tutela especial para o consentimento do titular dos dados - mesmo que esta não seja a única base legal para o uso de dados. O consentimento, conforme definido no Art. 5º, inciso IX, deve ser fornecido de forma livre, informada e inequívoca, garantindo que o titular compreenda plenamente as implicações do tratamento de seus dados. Isso inclui a finalidade específica para a qual os dados serão utilizados, o período de armazenamento e os direitos do titular em relação aos seus dados. Dessa forma, entende-se que o consentimento é restritivo, de modo que o agente não pode estender a autorização concedida para outras finalidades não previstas inicialmente, e que o titular pode revogar o consentimento a qualquer momento, conforme previsto no Art. 8º da LGPD.

Para \citeonline{TefferViola2020}, \textit{Livre} significa que o titular pode escolher a utilização de seus dados sem intervenções ou pressões externas que viciem o consentimento por meio de assimetria entre as partes. \textit{Informado} implica que o titular deve ter a sua disposição informações suficientes e acessíveis para avaliar a forma como seus dados serão tratados e os riscos e implicações do processo, levando em conta a assimetria técnica e informacional entre as partes. Já \textit{inequívoco} sugere que o consentimento deve ser expresso de maneira clara, sem ambiguidades, seja por meio de uma ação afirmativa ou declaração explícita, de modo que o ônus da prova de que o consentimento foi obtido de forma adequada recai sobre o agente de tratamento.

O cumprimento desses requisitos granulares, somado à necessidade de gerenciar a revogação do consentimento, impõe um significativo ônus operacional e burocrático às organizações. Essa definição rigorosa reflete a preocupação da LGPD em garantir que o titular mantenha o controle sobre seus dados pessoais, alinhando-se com o conceito de autodeterminação informativa de Westin discutida na seção anterior.

Apesar de que o consentimento não seja a única base legal para o tratamento de dados, ele é especialmente relevante no contexto de dados sensíveis, que requerem tutela especial. O Art. 11 da LGPD estabelece que o tratamento de dados sensíveis somente poderá ocorrer com o consentimento específico e destacado do titular, salvo em situações excepcionais previstas na lei, como cumprimento de obrigação legal ou regulatória, proteção da vida ou da incolumidade física do titular ou de terceiros, ou para a tutela da saúde, exclusivamente em procedimento realizado por profissionais de saúde, serviços de saúde ou autoridade sanitária.
Essas exceções reconhecem que há circunstâncias em que o tratamento de dados sensíveis é necessário para proteger interesses públicos ou direitos fundamentais, mesmo sem o consentimento do titular.

Como complemento ao consentimento, a LGPD estabelece o princípio da transparência como um de seus pilares fundamentais, conforme o Art. 6º, inciso VI. Este princípio garante aos titulares o acesso a "informações claras, precisas e facilmente acessíveis" sobre como seus dados são tratados. Na prática, a transparência é a ferramenta que viabiliza o consentimento verdadeiramente "informado". Sem que as organizações comuniquem abertamente quais dados são coletados, para qual finalidade e por quanto tempo, o titular não possui as condições necessárias para consentir de forma livre e consciente. Portanto, a combinação do consentimento (a ação do titular) com a transparência (o dever do controlador) busca reequilibrar a relação assimétrica entre as partes, fomentando a confiança no tratamento de dados pessoais.

\subsection{O Tratamento de Dados para Fins de Pesquisa na LGPD}
\label{subsec:fund-pesquisa-lgpd}

Como destacado na seção anterior, o tratamento de dados pessoais, como os dados clínicos, é fortemente regulado pela LGPD, exigindo o consentimento explícito do titular. No entanto, a lei também reconhece a importância do avanço científico e tecnológico para a sociedade, estabelecendo exceções específicas para o uso de dados pessoais sem consentimento em determinados contextos de pesquisa. Em seu Art. 7º, inciso IV, a lei estabelece uma base legal específica que autoriza o tratamento de dados pessoais "para a realização de estudos por órgão de pesquisa", dispensando a necessidade de outras bases legais como o consentimento \cite{Brasil2018lgpd}. Conforme o Art. 5º, XVIII, a definição de "órgão de pesquisa" é ampla, incluindo entidades da administração pública e organizações privadas sem fins lucrativos que tenham como missão institucional a pesquisa básica ou aplicada.

A LGPD é ainda mais específica ao tratar do uso de dados sensíveis no contexto da pesquisa científica. O Art. 11, inciso II, prevê que o tratamento de dados sensíveis pode ocorrer sem o consentimento do titular "para a realização de estudos por órgão de pesquisa, garantida, sempre que possível, a anonimização dos dados pessoais sensíveis" \cite{Brasil2018lgpd}. Essa disposição reconhece que a pesquisa científica muitas vezes depende do acesso a dados sensíveis para gerar conhecimento que pode beneficiar a sociedade como um todo. Ainda assim, a lei estabelece uma diretriz clara que preza pela desvinculação completa entre os dados e os indivíduos, buscando minimizar os riscos de violação de privacidade.

A anonimização dos dados, como especificada na LGPD, é a principal ferramenta para equilibrar a proteção da privacidade dos titulares dos dados com a necessidade de acesso a informações sensíveis para fins de pesquisa e desenvolvimento científico. Essa salvaguarda impões também um desafio técnico significativo para as instituições de pesquisa, que devem ser mais cautelares na forma como coletam, armazenam e processam dados sensíveis a fim de garantir uma anonimização que reduza o risco de reidentificação dos indivíduos ao mesmo tempo em que preserva a utilidade dos dados para análise científica.

\subsection{Anonimização vs. Pseudonimização sob a Ótica da Lei}
\label{subsec:fund-anon-pseudo}
% Apresente a definição técnica e jurídica de cada termo, deixando claro por que a
% anonimização efetiva é o objetivo para dispensar o consentimento.

Tratando-se de um instrumento essencial para a proteção da privacidade, a anonimização é um conceito que ganha atenção especial na LGPD. Conforme o Art. 5º, inciso XI, é definida como a "utilização de meios técnicos razoáveis e disponíveis no momento do tratamento, por meio dos quais um dado perde a possibilidade de associação, direta ou indireta, a um indivíduo". Ainda, a lei também faz a distinção com a pseudonimização, definida como um tratamento de segurança que, por meio da separação de informações, faz com que um dado perca a possibilidade de associação direta a um indivíduo, exceto pelo uso de informação adicional mantida separadamente pelo controlador \cite[Art. 13, § 4º]{Brasil2018lgpd}. Nesse sentido, determina-se que apesar de ambas as técnicas visarem a proteção da privacidade, a anonimização é um processo mais robusto, pois remove permanentemente a possibilidade de reidentificação, enquanto a pseudonimização apenas dificulta essa associação, mas não a elimina completamente.

Como consequência dessa distinção, entende-se que o dado pseudoanonimizado é identificável, e portanto continua sendo considerado um dado pessoal sob a LGPD cujo tratamento requer uma base legal, como o consentimento do titular. Já o dado anonimizado, por perder a possibilidade de associação a um indivíduo, deixa de ser considerado um dado pessoal e, portanto, pode ser tratado sem a necessidade de consentimento, desde que a anonimização seja efetiva e irreversível. Isso significa que, uma vez anonimizado, o conjunto de dados pode ser utilizado para pesquisa e outras finalidades sem as restrições impostas pela LGPD. Para o escopo deste trabalho, o foco será na anonimização, buscando um método de tratamento de dados que dissocie de forma irreversível os dados clínicos de seus titulares.

Do ponto de vista técnico, essa distinção pode ser formalizada. A anonimização pode ser entendida como uma função A que transforma um dado bruto X em um dado anonimizado X’, tal que não exista uma função de reidentificação R capaz de reverter o processo. Se essa função de reversão R existe, o processo é, na verdade, uma pseudonimização. Contudo, na prática, alcançar o que se pode chamar de "anonimização verdadeira" é um desafio complexo. A principal dificuldade reside no balanço inerente entre o nível de anonimato e a usabilidade dos dados: via de regra, à medida que o nível de anonimato aumenta, a utilidade dos dados para análise tende a diminuir. Com isso, a fronteira entre anonimização e pseudonimização deixa de ser uma linha clara e passa a ser uma questão de risco residual de reidentificação, que varia conforme a técnica utilizada e o contexto \cite{Zuo2021Data}.

\section{O Valor dos Dados Clínicos para Pesquisa e Inovação}
\label{sec:fund-valor-dados}

\subsection{A Importância do uso de dados na Saúde}
\label{subsec:fund-importancia-dados}
A atual pesquisa na área da saúde é marcada por uma transição fundamental, afastando-se de ensaios clínicos com amostras limitadas em direção a uma análise de grandes volumes de dados, aproximando-se do Big Data. Conforme caracterizado na literatura, o Big Data na saúde não se refere apenas ao imenso volume de informações, mas também à sua variedade, que abrange prontuários eletrônicos (EHRs), dados genômicos e de imagem, e à velocidade com que são gerados \cite{Belle2015}. Essa mudança permite a análise de dados do mundo real, que, apesar de seu vasto potencial, historicamente permaneceram subutilizados e isolados em diferentes sistemas, impedindo uma visão global e transparente da saúde dos pacientes \cite{Belle2015}.

Essa abundância de dados impulsionou a visão da Medicina de Precisão, que busca construir uma "Rede de Conhecimento" para integrar dados biológicos, clínicos e diagnósticos em uma escala sem precedentes \cite{NationalResearchCouncil2011}. O objetivo é compreender e utilizar as "idiossincrasias moleculares" de cada indivíduo, como as milhões de variações genéticas que influenciam o risco de doenças e a resposta a tratamentos. Para alcançar essa meta, é essencial agregar e analisar conjuntamente dados estruturados e não-estruturados de múltiplas fontes, como EHRs e dados genômicos, a fim de obter uma perspectiva mais completa e holística dos estados de saúde e doença \cite{Belle2015, NationalResearchCouncil2011}.

O potencial dessa abordagem é validado de forma contundente por estudos recentes que aplicam técnicas de Inteligência Artificial (IA) e Machine Learning (ML) a esses grandes datasets. Análises demonstram que modelos de ML, como Deep Learning (DL) e algoritmos de ensemble, superam consistentemente os métodos estatísticos tradicionais e os escores de risco convencionais. Na predição de risco cardiovascular, por exemplo, modelos de ML apresentam melhor performance que as ferramentas clássicas \cite{Liu2025}. Da mesma forma, no combate à resistência antimicrobiana (AMR), uma das maiores ameaças à saúde pública global, modelos de ML demonstraram maior acurácia e velocidade para prever a resistência de patógenos quando comparados a métodos tradicionais, que podem levar dias para gerar resultados \cite{Pennisi2025}.

Essa superioridade computacional se traduz em avanços práticos diretos. Na luta contra a AMR, por exemplo, os modelos de ML podem analisar em segundos o histórico do paciente, resultados de exames e dados microbiológicos para guiar a seleção de antibióticos com uma precisão inédita, otimizando o tratamento e combatendo o avanço da resistência \cite{Pennisi2025}. Essa capacidade de integrar e analisar rapidamente dados de imagem, biomarcadores e informações genéticas é o que permite que a visão da Medicina de Precisão saia do campo teórico, viabilizando a identificação de populações de alto risco e a recomendação de terapias personalizadas \cite{Liu2025, Pennisi2025}.

Contudo, a concretização desse potencial esbarra em desafios técnicos e éticos significativos, apontados de forma unânime pela literatura. A baixa qualidade, a falta de padronização e a heterogeneidade dos dados nos EHRs são barreiras técnicas que dificultam a generalização dos modelos \cite{Liu2025, Pennisi2025}. Além disso, a privacidade e a segurança dos dados são premissas fundamentais. A literatura destaca a gestão da privacidade como uma das questões mais críticas, dado o risco de discriminação a partir da exposição de informações sensíveis sobre estilo de vida, condições de saúde e predisposições genéticas \cite{Belle2015, NationalResearchCouncil2011, Pennisi2025}. Portanto, o desenvolvimento de técnicas que permitam o uso desses dados para pesquisa, garantindo a proteção contra a reidentificação e o respeito aos direitos dos titulares, é a condição indispensável para que a promessa do Big Data na saúde se realize de forma ética e segura.

\subsection{Estrutura e Atributos dos Dados Clínicos para Anonimização}
\label{subsec:fund-dados-categorizacao}

Para aplicar técnicas de anonimização de forma eficaz, é importante compreender tanto a estrutura dos dados clínicos quanto a natureza de seus atributos. A literatura sobre privacidade de dados frequentemente parte do modelo relacional (ou tabular), no qual as informações são organizadas em tabelas com linhas representando registros (ex: uma consulta médica) e colunas representando atributos (ex: diagnóstico, data) \cite{Olatunji2024}. Esse formato é a base dos prontuários eletrônicos (PEPs) e de muitos registros de doenças, sendo o ponto de partida para a maioria dos modelos de anonimização \cite{Zuo2021Data}.

Contudo, os dados de saúde frequentemente ultrapassam o modelo relacional estático, apresentando estruturas mais complexas que desafiam a anonimização. Isso inclui dados transacionais e sequenciais, cuja natureza incremental exige técnicas dinâmicas de proteção \cite{Zuo2021Data, Olatunji2024}. A complexidade atinge seu ápice nos dados em grafo, que modelam as relações entre entidades (pacientes, médicos). Nesses casos, o risco à privacidade não reside apenas nos atributos dos indivíduos, mas nas próprias conexões, que podem ser exploradas para reidentificar um titular através dos chamados "ataques de vizinhança" \cite{Olatunji2024}.

Independentemente da estrutura do dado, seus atributos devem ser categorizados para a correta aplicação das técnicas de anonimização. A literatura consolida uma classificação de três tipos principais de atributos de risco. Primeiro, os Identificadores Diretos (ID), que explicitamente identificam um indivíduo, como o número do CPF ou uma combinação de nome e endereço. Em segundo lugar, os Quasi-Identificadores (QI), que são atributos que, isoladamente, não identificam ninguém, mas quando combinados podem levar à reidentificação com alta probabilidade, como gênero, data de nascimento e código postal \cite{Zuo2021Data, Olatunji2024}.

Por fim, existem os Atributos Sensíveis (AS), que contêm as informações que se deseja proteger (ex: diagnóstico, dosagem de um medicamento), mas que não são diretamente úteis para a reidentificação \cite{Zuo2021Data}. É crucial notar que os Quasi-Identificadores frequentemente são também os atributos de maior valor para a análise científica, criando o dilema central da anonimização: como modificar ou suprimir os QIs para proteger a privacidade sem destruir a utilidade dos dados \cite{Zuo2021Data}. Compreender essa dupla categorização — tanto a estrutura do dado (relacional, grafo, etc.) quanto o tipo de atributo (ID, QI, AS) — é, portanto, o passo essencial que antecede a escolha e a aplicação de qualquer modelo de anonimização, pois diferentes estruturas e atributos exigem diferentes estratégias de proteção.

% --- Início da Seção: Métodos e Técnicas de Anonimização de Dados ---

\section{Métodos e Técnicas de Anonimização de Dados}
\label{sec:fund-metodos-anon}

A aplicação de qualquer método de anonimização busca atingir um balanço entre três objetivos principais: preservar a \textbf{privacidade} do titular, garantir a \textbf{utilidade} dos dados para análise e manter a \textbf{veracidade} das informações, assegurando que cada registro anonimizado corresponda a um registro original \cite{Olatunji2024}. Contudo, a literatura é unânime em afirmar que não existe um método universal, aplicável a todos os cenários. A escolha da abordagem correta depende da estrutura dos dados, dos riscos de privacidade específicos e do nível de utilidade que se deseja manter \cite{Vovk2023}. A seguir, serão discutidos os principais paradigmas de anonimização, começando pelos modelos que se tornaram a base para o campo.

% ---
\subsection{Proteção Contra a Divulgação de Identidade: O k-Anonimato}
\label{subsec:fund-k-anonimato}

O modelo de privacidade mais conhecido e fundamental é o \textbf{k-anonimato}, cujo objetivo é proteger contra a divulgação de identidade (\textit{identity disclosure}), ou seja, a ligação direta de um registro a um indivíduo \cite{Sepas2022, Vovk2023}. Um conjunto de dados satisfaz o k-anonimato se cada registro for indistinguível de pelo menos $k-1$ outros registros no que diz respeito ao conjunto de Quasi-Identificadores (QI) \cite{Zuo2021Data, Olatunji2024}. Na prática, isso é alcançado por meio de técnicas de \textbf{generalização} (ex: substituir a idade "34" pelo intervalo "30-40") e \textbf{supressão} (ex: remover um registro atípico, ou outlier, que comprometeria a formação de um grupo) \cite{Sepas2022}. A formalização simples do modelo é:
$$L_{KA} = k, \text{ tal que } \forall E : |E| \geq k$$
Onde $E$ representa uma classe de equivalência (o grupo de registros indistinguíveis).

Contudo, apesar de sua importância fundamental, o k-anonimato possui vulnerabilidades conhecidas. A primeira é a "maldição da dimensionalidade", onde o custo computacional e a perda de informação aumentam exponencialmente com a quantidade de atributos (QIs) a serem anonimizados \cite{Zuo2021Data}. Mais importante, o modelo é suscetível a dois ataques principais. O \textbf{ataque de homogeneidade} ocorre quando todos os registros em uma classe de equivalência possuem o mesmo valor para um atributo sensível, permitindo a inferência com 100\% de certeza. O \textbf{ataque de conhecimento} (\textit{background knowledge attack}) ocorre quando um adversário utiliza informações externas para reduzir as possibilidades dentro de um grupo e inferir o atributo sensível \cite{Zuo2021Data, Vovk2023}. Essas limitações motivaram o desenvolvimento de modelos mais robustos.

% ---
\subsection{Refinando a Proteção: l-Diversidade e t-Closeness}
\label{subsec:fund-l-diversidade}

Para endereçar as vulnerabilidades do k-anonimato, especificamente o ataque de homogeneidade, o modelo de \textbf{l-diversidade} foi proposto para proteger contra a divulgação de atributos (\textit{attribute disclosure}) \cite{Sepas2022}. A l-diversidade exige que cada classe de equivalência (o grupo k-anônimo) contenha pelo menos '$l$' valores distintos para o atributo sensível \cite{Zuo2021Data, Olatunji2024}. Essa garantia de diversidade interna impede a inferência direta que era possível no modelo anterior. A versão mais comum, a Entropia l-Diversidade, é formalizada como:
$$L_{ELD} = l, \text{ tal que } \forall E : H(S_E) \geq \log(l)$$
Onde $H(S_E)$ é a entropia da distribuição do atributo sensível $S$ na classe de equivalência $E$.

Ainda assim, a l-diversidade também apresenta vulnerabilidades. O modelo não protege contra o \textbf{ataque de similaridade semântica}, onde os '$l$' valores distintos podem ser conceitualmente próximos (ex: um grupo com os diagnósticos "gastrite", "úlcera gástrica" e "câncer de estômago" permite inferir que o indivíduo possui um problema gástrico) \cite{Olatunji2024}. Além disso, o modelo é vulnerável ao \textbf{ataque de assimetria} (\textit{skewness attack}), quando a distribuição geral de um atributo sensível é muito diferente da distribuição dentro de um grupo específico, permitindo inferências estatísticas \cite{Zuo2021Data}.

Para mitigar essas falhas, foi introduzido o modelo de \textbf{t-closeness}. Ele refina a l-diversidade ao exigir que a distribuição de probabilidade do atributo sensível dentro de cada classe de equivalência seja próxima (a uma distância máxima de '$t$') da distribuição desse mesmo atributo no conjunto de dados completo \cite{Zuo2021Data, Olatunji2024}. Ao forçar que a distribuição local imite a distribuição global, o t-closeness impede que um adversário ganhe muita informação ao descobrir a qual grupo um indivíduo pertence, oferecendo uma proteção mais robusta contra a divulgação de atributos.

\begin{table}[h!]
\centering
\caption{Comparativo dos principais modelos de anonimização para dados relacionais.}
\label{tab:modelos-comparativo}
\begin{tabular}{|p{0.2\textwidth}|p{0.35\textwidth}|p{0.35\textwidth}|}
\hline
\textbf{Modelo} & \textbf{Protege Contra} & \textbf{Vulnerabilidades Principais} \\ \hline
\textbf{k-Anonimato} & Divulgação de Identidade. & Ataque de homogeneidade e de conhecimento. Perda de utilidade em dados de alta dimensionalidade. \\ \hline
\textbf{l-Diversidade} & Divulgação de Atributos (ao garantir diversidade de valores sensíveis). & Ataque de similaridade semântica e de assimetria. Dificuldade de aplicação em dados desbalanceados. \\ \hline
\textbf{t-Closeness} & Divulgação de Atributos (ao garantir similaridade de distribuições). & Custo computacional elevado e pode ser excessivamente restritivo, levando a uma grande perda de informação. \\ \hline
\end{tabular}
\end{table}

% ---
\subsection{Outros Paradigmas e Técnicas Operacionais}
\label{subsec:fund-outros-paradigmas}

Além dos modelos baseados em generalização, a literatura descreve outros paradigmas para alcançar a anonimização. Uma abordagem fundamental é a \textbf{perturbação}, que consiste em substituir os valores originais, sejam eles QIs ou Atributos Sensíveis (AS), por "máscaras falsas" ou dados sintéticos gerados a partir de uma distribuição de ruído \cite{Olatunji2024}. O objetivo dessa técnica não é tornar os registros indistinguíveis, mas sim mascarar os dados verdadeiros de um indivíduo de tal forma que, embora a informação individual seja ofuscada, as propriedades estatísticas agregadas do conjunto de dados permaneçam úteis para análise \cite{Vovk2023}.

Outra família de técnicas opera através da \textbf{partição dos dados}. O \textit{slicing} (fatiamento), por exemplo, particiona o dataset tanto verticalmente (agrupando atributos correlacionados em colunas) quanto horizontalmente (agrupando tuplas em "baldes" ou \textit{buckets}), dissociando atributos que, juntos, poderiam levar à reidentificação \cite{Olatunji2024}. De forma similar, a \textit{bucketization} separa os QIs dos Atributos Sensíveis; os QIs são mantidos em sua forma original, mas os AS são embaralhados aleatoriamente dentro de cada \textit{bucket}. Embora essa técnica ofereça alta utilidade por preservar os QIs, ela apresenta uma vulnerabilidade crítica: como os QIs não são alterados, a \textit{bucketization} não protege contra a divulgação de participação (\textit{membership disclosure}), ou seja, um adversário ainda pode verificar se o registro de um indivíduo está presente no dataset \cite{Olatunji2024}.

Técnicas de \textbf{agregação e refinamento} também são amplamente utilizadas. A \textbf{microagregação}, por exemplo, é aplicada a atributos numéricos e funciona agrupando registros em pequenos clusters de tamanho mínimo $k$ e, em seguida, substituindo os valores de cada registro pelo centroide (média) do seu cluster \cite{Olatunji2024}. Já a \textbf{supressão}, frequentemente usada para evitar a generalização excessiva causada por outliers, pode ser aplicada em nível de registro (removendo a linha inteira, o que pode levar a uma perda excessiva de informação) ou em nível de célula (removendo apenas os valores problemáticos de QIs). A supressão em nível de célula é geralmente preferível por preservar melhor a utilidade dos dados, tratando os valores suprimidos como dados faltantes, algo que muitos modelos de machine learning conseguem acomodar \cite{Olatunji2024}.

Por fim, uma abordagem conceitualmente distinta é o uso de \textbf{algoritmos criptográficos}. Em vez de modificar os dados, este paradigma utiliza funções de mão única (irreversíveis), como o \textit{hashing}, para mapear cada registro original para um novo registro único e não identificável. A premissa é que a função não pode ser revertida para se obter o dado original, garantindo a proteção da identidade \cite{Vovk2023}. Estudos indicam que essa abordagem pode alcançar um excelente balanço entre privacidade e utilidade, com perda de informação minimizada em comparação com técnicas baseadas em k-anonimato. Contudo, sua implementação prática é um desafio, geralmente exigindo maior poder computacional, mais espaço de armazenamento e uma infraestrutura mais complexa para ser realizada de forma eficaz \cite{Vovk2023}.

% ---
\subsection{O Modelo da Privacidade Diferencial}
\label{subsec:fund-privacidade-diferencial}

Um paradigma mais moderno e que oferece uma garantia de privacidade mais forte e matemática é a \textbf{Privacidade Diferencial (DP)}. Diferente dos modelos anteriores que se focam em propriedades do dataset anonimizado, a DP é uma propriedade de um algoritmo de análise. A sua intuição fundamental é garantir que a inclusão ou exclusão de qualquer registro individual no conjunto de dados tenha um impacto estatisticamente insignificante no resultado de qualquer consulta ou análise realizada \cite{Olatunji2024}. Essa garantia robustece a proteção contra ataques de reconstrução e inferência.

A privacidade diferencial é formalmente definida como $\epsilon$-Privacidade Diferencial, onde $\epsilon$ (\textit{epsilon}) é o parâmetro de privacidade que quantifica a "perda de privacidade" máxima permitida por uma consulta. Um valor de $\epsilon$ menor implica em maior privacidade (e geralmente mais ruído adicionado), enquanto um valor maior implica em menor privacidade e maior precisão nos resultados. A definição formal é:
$$\text{Pr}[A(X_1) \in S] \leq e^{\epsilon} \times \text{Pr}[A(X_2) \in S]$$
Onde $A$ é um algoritmo randomizado, $X_1$ e $X_2$ são datasets vizinhos (diferem em apenas um registro), e $S$ é um possível resultado. O ruído para satisfazer a DP pode ser adicionado de forma \textbf{local} (em cada dado individual antes de ser coletado) ou \textbf{global} (no resultado da consulta por um curador de dados confiável), sendo a abordagem global geralmente mais acurada \cite{Olatunji2024}.

A escolha de um modelo de anonimização, portanto, envolve uma análise complexa. Conforme a literatura aponta, é impossível eliminar completamente o risco de reidentificação, e a maioria dos métodos é computacionalmente custosa \cite{Sepas2022}. O principal desafio, reiterado em múltiplos estudos, é encontrar o "equilíbrio delicado" entre a proteção da privacidade e a utilidade dos dados \cite{Vovk2023, Olatunji2024}. Uma anonimização mais forte geralmente implica em maior perda de informação, o que pode impactar negativamente a qualidade das análises e dos modelos de machine learning. Essa tensão fundamental entre privacidade e utilidade exige que, além de aplicar uma técnica, seja possível medir e avaliar o resultado de ambos os aspectos.

% --- Fim da Seção ---
\section{Métricas de Avaliação: O Balanço entre Privacidade e Utilidade}
\label{sec:fund-metricas}

\subsection{Avaliação do Risco de Reidentificação (Métricas de Privacidade)}
\label{subsec:fund-metricas-privacidade}
% Descreva como se pode quantificar o risco de um indivíduo ser reidentificado em um dataset
% anonimizado. Apresente algumas métricas ou modelos de risco.

\subsection{Avaliação da Utilidade dos Dados Anonimizados (Métricas de Qualidade)}
\label{subsec:fund-metricas-utilidade}
% Detalhe as duas abordagens principais para medir a qualidade/utilidade dos dados:
% 1. Similaridade estatística (comparação de distribuições, correlações, etc.).
% 2. Performance em tarefas de Machine Learning (comparando a performance de um modelo
%    treinado nos dados originais vs. nos dados anonimizados).
% ---

% ---
\section{Síntese da Fundamentação e Escolha da Abordagem}
\label{sec:fund-sintese}
% Feche o capítulo com um ou dois parágrafos que resumam o que foi apresentado e, o mais importante,
% justifique quais técnicas e métricas discutidas aqui serão empregadas na parte prática do seu TCC.
% Isso cria a ponte perfeita para o seu próximo capítulo (Metodologia/Desenvolvimento).
% ---